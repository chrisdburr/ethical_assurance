
@techreport{adalovelaceinstitute2020,
  title = {Examining the {{Black Box}}},
  author = {Ada Lovelace Institute, DataKind UK},
  year = {2020},
  month = apr,
  file = {/Users/cburr/Zotero/storage/5AVC5ALY/Ada-Lovelace-Institute-DataKind-UK-Examining-the-Black-Box-Report-2020.pdf}
}

@book{arendt1998,
  title = {The Human Condition},
  author = {Arendt, Hannah and Canovan, Margaret},
  year = {1998},
  publisher = {{University of Chicago Press}},
  file = {/Users/cburr/Zotero/storage/BGVFKUCJ/Arendt and Canovan - 1998 - The human condition.pdf}
}

@article{arnold2018,
  title = {{{FactSheets}}: {{Increasing Trust}} in {{AI Services}} through {{Supplier}}'s {{Declarations}} of {{Conformity}}},
  shorttitle = {{{FactSheets}}},
  author = {Arnold, Matthew and Bellamy, Rachel K. E. and Hind, Michael and Houde, Stephanie and Mehta, Sameep and Mojsilovic, Aleksandra and Nair, Ravi and Ramamurthy, Karthikeyan Natesan and Reimer, Darrell and Olteanu, Alexandra and Piorkowski, David and Tsay, Jason and Varshney, Kush R.},
  year = {2018},
  month = aug,
  abstract = {Accuracy is an important concern for suppliers of artificial intelligence (AI) services, but considerations beyond accuracy, such as safety (which includes fairness and explainability), security, and provenance, are also critical elements to engender consumers' trust in a service. Many industries use transparent, standardized, but often not legally required documents called supplier's declarations of conformity (SDoCs) to describe the lineage of a product along with the safety and performance testing it has undergone. SDoCs may be considered multi-dimensional fact sheets that capture and quantify various aspects of the product and its development to make it worthy of consumers' trust. Inspired by this practice, we propose FactSheets to help increase trust in AI services. We envision such documents to contain purpose, performance, safety, security, and provenance information to be completed by AI service providers for examination by consumers. We suggest a comprehensive set of declaration items tailored to AI and provide examples for two fictitious AI services in the appendix of the paper.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/AKTXJCSY/Arnold et al. - 2018 - FactSheets Increasing Trust in AI Services throug.pdf;/Users/cburr/Zotero/storage/9KN6DS48/1808.html}
}

@article{ashmore2019,
  title = {Assuring the {{Machine Learning Lifecycle}}: {{Desiderata}}, {{Methods}}, and {{Challenges}}},
  shorttitle = {Assuring the {{Machine Learning Lifecycle}}},
  author = {Ashmore, Rob and Calinescu, Radu and Paterson, Colin},
  year = {2019},
  month = may,
  journal = {arXiv:1905.04223 [cs, stat]},
  eprint = {1905.04223},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine learning has evolved into an enabling technology for a wide range of highly successful applications. The potential for this success to continue and accelerate has placed machine learning (ML) at the top of research, economic and political agendas. Such unprecedented interest is fuelled by a vision of ML applicability extending to healthcare, transportation, defence and other domains of great societal importance. Achieving this vision requires the use of ML in safety-critical applications that demand levels of assurance beyond those needed for current ML applications. Our paper provides a comprehensive survey of the state-of-the-art in the assurance of ML, i.e. in the generation of evidence that ML is sufficiently safe for its intended use. The survey covers the methods capable of providing such evidence at different stages of the machine learning lifecycle, i.e. of the complex, iterative process that starts with the collection of the data used to train an ML component for a system, and ends with the deployment of that component within the system. The paper begins with a systematic presentation of the ML lifecycle and its stages. We then define assurance desiderata for each stage, review existing methods that contribute to achieving these desiderata, and identify open challenges that require further research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,Statistics - Machine Learning},
  file = {/Users/cburr/Zotero/storage/KX6IHHBL/Ashmore et al. - 2019 - Assuring the Machine Learning Lifecycle Desiderat.pdf}
}

@article{atkinson2017,
  title = {Toward {{Artificial Argumentation}}},
  author = {Atkinson, Katie and Baroni, Pietro and Giacomin, Massimiliano and Hunter, Anthony and Prakken, Henry and Reed, Chris and Simari, Guillermo and Thimm, Matthias and Villata, Serena},
  year = {2017},
  journal = {AI Magazine},
  pages = {25--36},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/NEU84NNF/Atkinson et al. - Toward Artificial Argumentation.pdf}
}

@article{bailey2002,
  title = {{Mind the gap! Comparing ex ante and ex post assessments of the costs of complying with environmental regulation}},
  author = {Bailey, Peter D. and Haq, Gary and Gouldson, Andy},
  year = {2002},
  journal = {European Environment},
  volume = {12},
  number = {5},
  pages = {245--256},
  issn = {1099-0976},
  doi = {10.1002/eet.303},
  abstract = {This paper considers the question of how ex ante predictions of the costs of complying with environmental regulations compare with ex post evaluations of actual compliance costs. This is an important issue given the emergence of requirements for regulatory impact assessment (RIA) in many policy arenas (see OECD, 1997) and the conclusion of previous research that the predicted costs of compliance with environmental regulations often exceed actual costs (see Haq et al., 2001). Based upon a review of the different stages of the regulatory decision-making process, this paper suggests that the reasons for these differences relate to strategic behaviour by affected parties in the policy formulation stage, problems in anticipating the influence of the implementation process, difficulties in forecasting the availability of new technological solutions and incentives for firms to reduce the costs of compliance once environmental regulations have been adopted. Copyright \textcopyright{} 2002 John Wiley \& Sons, Ltd. and ERP Environment.},
  langid = {french},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/eet.303},
  file = {/Users/cburr/Zotero/storage/WBH3XCLJ/Bailey et al. - 2002 - Mind the gap! Comparing ex ante and ex post assess.pdf;/Users/cburr/Zotero/storage/UMGLRCW9/eet.html}
}

@techreport{balaram2018,
  title = {Artificial {{Intelligence}}: {{Real Public Engageme}}},
  author = {Balaram, Brhmie and Greenham, Tony and Leonard, Jasmine},
  year = {2018},
  month = may,
  institution = {{Royal Society for the encouragement of Arts, Manufactures and Commerce}},
  file = {/Users/cburr/Zotero/storage/PLWI9ZG7/rsa_artificial-intelligence---real-public-engagement.pdf}
}

@article{bender2018,
  title = {Data {{Statements}} for {{Natural Language Processing}}: {{Toward Mitigating System Bias}} and {{Enabling Better Science}}},
  shorttitle = {Data {{Statements}} for {{Natural Language Processing}}},
  author = {Bender, Emily M. and Friedman, Batya},
  year = {2018},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {6},
  pages = {587--604},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00041},
  abstract = {In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/7WIHLGJ3/Bender and Friedman - 2018 - Data Statements for Natural Language Processing T.pdf}
}

@article{blastland2020,
  title = {Five Rules for Evidence Communication},
  author = {Blastland, Michael and Freeman, Alexandra L. J. and van der Linden, Sander and Marteau, Theresa M. and Spiegelhalter, David},
  year = {2020},
  month = nov,
  journal = {Nature},
  volume = {587},
  number = {7834},
  pages = {362--364},
  doi = {10.1038/d41586-020-03189-1},
  abstract = {Avoid unwarranted certainty, neat narratives and partisan presentation; strive to inform, not persuade.},
  copyright = {2020 Nature},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/DC2R62AM/Blastland et al. - 2020 - Five rules for evidence communication.pdf}
}

@incollection{bloomfield2010,
  title = {Safety and {{Assurance Cases}}: {{Past}}, {{Present}} and {{Possible Future}} \textendash{} an {{Adelard Perspective}}},
  shorttitle = {Safety and {{Assurance Cases}}},
  booktitle = {Making {{Systems Safer}}},
  author = {Bloomfield, Robin and Bishop, Peter},
  editor = {Dale, Chris and Anderson, Tom},
  year = {2010},
  pages = {51--67},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-84996-086-1_4},
  abstract = {This paper focuses on the approaches used in safety cases for software based systems. We outline the history of approaches for assuring the safety of software-based systems, the current uptake of safety and assurance cases and the current practice on structured safety cases. Directions for further development are discussed.},
  isbn = {978-1-84996-085-4 978-1-84996-086-1},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/USJKY6XN/Bloomfield and Bishop - 2010 - Safety and Assurance Cases Past, Present and Poss.pdf}
}

@misc{burr2021,
  title = {Ethical {{Assurance}}: {{A}} Practical Approach to the Responsible Design, Development, and Deployment of Data-Driven Technologies},
  author = {Burr, Christopher and Leslie, David},
  year = {2021},
  eprint = {2110.05164},
  eprinttype = {arxiv},
  primaryclass = {cs.CY},
  archiveprefix = {arXiv}
}

@article{burton2020,
  title = {Mind the Gaps: {{Assuring}} the Safety of Autonomous Systems from an Engineering, Ethical, and Legal Perspective},
  shorttitle = {Mind the Gaps},
  author = {Burton, Simon and Habli, Ibrahim and Lawton, Tom and McDermid, John and Morgan, Phillip and Porter, Zoe},
  year = {2020},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {279},
  pages = {103201},
  issn = {00043702},
  doi = {10.1016/j.artint.2019.103201},
  abstract = {This paper brings together a multi-disciplinary perspective from systems engineering, ethics, and law to articulate a common language in which to reason about the multifaceted problem of assuring the safety of autonomous systems. The paper's focus is on the ``gaps'' that arise across the development process: the semantic gap, where normal conditions for a complete specification of intended functionality are not present; the responsibility gap, where normal conditions for holding human actors morally responsible for harm are not present; and the liability gap, where normal conditions for securing compensation to victims of harm are not present. By categorising these ``gaps'' we can expose with greater precision key sources of uncertainty and risk with autonomous systems. This can inform the development of more detailed models of safety assurance and contribute to more effective risk control.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/KHHSVEID/Burton et al. - 2020 - Mind the gaps Assuring the safety of autonomous s.pdf}
}

@article{calinescu2018,
  title = {Engineering {{Trustworthy Self-Adaptive Software}} with {{Dynamic Assurance Cases}}},
  author = {Calinescu, Radu and Weyns, Danny and Gerasimou, Simos and Iftikhar, Muhammad Usman and Habli, Ibrahim and Kelly, Tim},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Software Engineering},
  volume = {44},
  number = {11},
  pages = {1039--1069},
  issn = {1939-3520},
  doi = {10.1109/TSE.2017.2738640},
  abstract = {Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.},
  keywords = {Adaptive systems,assurance cases,assurance evidence,Computer architecture,Control systems,control theory,e-finance domains,embedded software,embedded systems,engineering of trustworthy self-adaptive software systems,engineering trustworthy,ENTRUST,formal verification,industry-adopted assurance processes,Monitoring,nonfunctional requirements,oceanic monitoring,Runtime,self-adaptive software handles,Self-adaptive software systems,service-based systems,software architecture,software engineering methodology,Software systems,trusted computing},
  file = {/Users/cburr/Zotero/storage/JXNVPT5I/Calinescu et al. - 2018 - Engineering Trustworthy Self-Adaptive Software wit.pdf;/Users/cburr/Zotero/storage/38IEHBCD/8008800.html}
}

@book{cartwright2012,
  title = {Evidence-Based Policy: {{A}} Practical Guide to Doing It Better},
  author = {Cartwright, Nancy and Hardie, Jeremy},
  year = {2012},
  publisher = {{Oxford University Press}}
}

@techreport{cleland2012,
  title = {Evidence: Using Safety Cases in Industry and Healthcare},
  shorttitle = {Evidence},
  author = {Cleland, George M and Habli, Ibrahim and Medhurst, John and {Health Foundation (Great Britain)}},
  year = {2012},
  langid = {english},
  annotation = {OCLC: 883415868},
  file = {/Users/cburr/Zotero/storage/JJADZSTG/Cleland et al. - 2012 - Evidence using safety cases in industry and healt.pdf}
}

@book{cleland2012a,
  title = {Evidence: Using Safety Cases in Industry and Healthcare},
  shorttitle = {Evidence},
  author = {Cleland, George M and Habli, Ibrahim and Medhurst, John and {Health Foundation (Great Britain)}},
  year = {2012},
  isbn = {978-1-906461-43-0},
  langid = {english},
  annotation = {OCLC: 883415868},
  file = {/Users/cburr/Zotero/storage/I5MHMEAP/Cleland et al. - 2012 - Evidence using safety cases in industry and healt.pdf}
}

@inproceedings{cobbe2021,
  title = {Reviewable {{Automated Decision-Making}}: {{A Framework}} for {{Accountable Algorithmic Systems}}},
  shorttitle = {Reviewable {{Automated Decision-Making}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Cobbe, Jennifer and Lee, Michelle Seng Ah and Singh, Jatinder},
  year = {2021},
  month = mar,
  pages = {598--609},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3442188.3445921},
  abstract = {This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review \textendash{} both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/W4BZ36DF/Cobbe et al. - 2021 - Reviewable Automated Decision-Making A Framework .pdf}
}

@article{denney2018,
  title = {Tool Support for Assurance Case Development},
  author = {Denney, Ewen and Pai, Ganesh},
  year = {2018},
  month = sep,
  journal = {Automated Software Engineering},
  volume = {25},
  number = {3},
  pages = {435--499},
  issn = {0928-8910, 1573-7535},
  doi = {10.1007/s10515-017-0230-5},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/5K3SMR5Z/Denney and Pai - 2018 - Tool support for assurance case development.pdf}
}

@book{eemeren2004,
  title = {A {{Systematic Theory}} of {{Argumentation}}: {{The}} Pragma-Dialectical Approach},
  author = {Eemeren, Frans H Van and Grootendorst, Rob},
  year = {2004},
  publisher = {{Cambridge University Press}},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/88BZQSUW/Eemeren and Grootendorst - A Systematic Theory of Argumentation The pragma-d.pdf}
}

@inproceedings{finnegan2014,
  title = {A {{Security Argument Pattern}} for {{Medical Device Assurance Cases}}},
  booktitle = {2014 {{IEEE International Symposium}} on {{Software Reliability Engineering Workshops}}},
  author = {Finnegan, Anita and McCaffery, Fergal},
  year = {2014},
  month = nov,
  pages = {220--225},
  doi = {10.1109/ISSREW.2014.89},
  abstract = {Medical device security is a growing concern for medical device manufacturers, healthcare delivery organisations and regulators in the industry. Increasingly, researchers are demonstrating exactly how vulnerable these devices are. In many cases, networked medical devices are regarded as a potential weak link within a healthcare IT network that could provide a means to expose the entire network to a malware attack. At present there is no formal method for implementing security risk management practices in the medical device industry. However, with new regulatory guidance being developed by the Food and Drug Administration (FDA), medical devices manufacturers will need to prove that their devices are secure. This paper presents a security case framework that is currently under development. The purpose of this framework is to provide medical device manufacturers and healthcare delivery organisations with a solution to assist both in establishing confidence in the security assurance of medical devices and to also maintain this confidence throughout the lifetime of the device.},
  keywords = {assurance cases,biomedical equipment,cybersecurity,FDA,Food and Drug Administration,health care,healthcare delivery organisations,healthcare IT network,IEC standards,invasive software,ISO standards,malware attack,medical computing,medical device assurance cases,medical device industry,medical device manufacturers,medical device security,Medical diagnostic imaging,Medical services,networked medical devices,regulatory guidance,risk management,Safety,Security,security argument pattern,security capability argument pattern,security cases,security risk management practices},
  file = {/Users/cburr/Zotero/storage/VS23J3UI/Finnegan and McCaffery - 2014 - A Security Argument Pattern for Medical Device Ass.pdf;/Users/cburr/Zotero/storage/G3XNCX6A/6983842.html}
}

@article{floridi2020,
  title = {Artificial {{Intelligence}} as a {{Public Service}}: {{Learning}} from {{Amsterdam}} and {{Helsinki}}},
  shorttitle = {Artificial {{Intelligence}} as a {{Public Service}}},
  author = {Floridi, Luciano},
  year = {2020},
  month = oct,
  journal = {Philosophy \& Technology},
  pages = {s13347-020-00434-3},
  issn = {2210-5433, 2210-5441},
  doi = {10.1007/s13347-020-00434-3},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/2HZDD9IF/Floridi - 2020 - Artificial Intelligence as a Public Service Learn.pdf}
}

@book{fukuyama1992,
  title = {The End of History and the Last Man},
  author = {Fukuyama, Francis},
  year = {1992},
  publisher = {{Free Press}},
  address = {{New York}},
  file = {/Users/cburr/Zotero/storage/XLQZRFQA/Fukuyama - 1992 - The end of history and the last man.pdf}
}

@article{gebru2019,
  title = {Datasheets for {{Datasets}}},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  year = {2019},
  journal = {arXiv:1803.09010 [cs]},
  eprint = {1803.09010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The machine learning community currently has no standardized process for documenting datasets. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  file = {/Users/cburr/Zotero/storage/98R5APXM/Gebru et al. - 2020 - Datasheets for Datasets.pdf}
}

@incollection{graydon2014,
  title = {Towards a {{Clearer Understanding}} of {{Context}} and {{Its Role}} in {{Assurance Argument Confidence}}},
  booktitle = {Computer {{Safety}}, {{Reliability}}, and {{Security}}},
  author = {Graydon, Patrick John},
  editor = {Bondavalli, Andrea and Di Giandomenico, Felicita},
  year = {2014},
  volume = {8666},
  pages = {139--154},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10506-2_10},
  abstract = {The Goal Structuring Notation (GSN) is a popular graphical notation for recording safety arguments. One of GSN's key innovations is a context element that links short phrases used in the argument to detail available elsewhere. However, definitions of the context element admit multiple interpretations and conflict with guidance for building assured safety arguments. If readers do not share an understanding of the meaning of context that makes context's impact on the main safety claim clear, confidence in safety might be misplaced. In this paper, we analyse the definitions and usage of GSN context elements, identify contradictions and vagueness, propose a more precise definition, and make updated recommendations for assured safety argument structure.},
  isbn = {978-3-319-10505-5 978-3-319-10506-2},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/PGBEEXNH/Graydon - 2014 - Towards a Clearer Understanding of Context and Its.pdf}
}

@article{habli2018,
  title = {What Is the Safety Case for Health {{IT}}? {{A}} Study of Assurance Practices in {{England}}},
  author = {Habli, Ibrahim and White, Sean and Sujan, Mark and Harrison, Stuart and Ugarte, Marta},
  year = {2018},
  journal = {Safety Science},
  volume = {110},
  pages = {324--335},
  doi = {10.1016/j.ssci.2018.09.001},
  file = {/Users/cburr/Zotero/storage/GZD5WPQC/1-s2.0-S0925753517310238-main.pdf}
}

@article{habli2020,
  title = {Enhancing {{COVID-19}} Decision Making by Creating an Assurance Case for Epidemiological Models},
  author = {Habli, Ibrahim and Alexander, Rob and Hawkins, Richard and Sujan, Mark and McDermid, John and Picardi, Chiara and Lawton, Tom},
  year = {2020},
  journal = {BMJ Health \&amp; Care Informatics},
  volume = {27},
  number = {3},
  pages = {e100165},
  doi = {10.1136/bmjhci-2020-100165},
  file = {/Users/cburr/Zotero/storage/NL7U8FBI/e100165.full.pdf}
}

@inproceedings{hawkins2009,
  title = {Software Safety Assurance - What Is Sufficient?},
  booktitle = {4th {{IET International Conference}} on {{Systems Safety}} 2009. {{Incorporating}} the {{SaRS Annual Conference}}},
  author = {Hawkins, R.D. and Kelly, T.P.},
  year = {2009},
  pages = {2A3-2A3},
  publisher = {{IET}},
  address = {{London, UK}},
  doi = {10.1049/cp.2009.1542},
  abstract = {It is possible to construct a safety argument for the software aspects of a system in order to demonstrate that the software is acceptably safe to operate. In order to be compelling, it is necessary to justify that the arguments and evidence presented for the software provide sufficient safety assurance. In this paper we consider how assurance may be explicitly considered when developing a software safety argument. We propose a framework for making and justifying decisions about the arguments and evidence required to assure the safety of the software.},
  isbn = {978-1-84919-195-1},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/IHJ5R2XY/Hawkins and Kelly - 2009 - Software safety assurance - what is sufficient.pdf}
}

@incollection{hawkins2011,
  title = {A {{New Approach}} to Creating {{Clear Safety Arguments}}},
  booktitle = {Advances in {{Systems Safety}}},
  author = {Hawkins, Richard and Kelly, Tim and Knight, John and Graydon, Patrick},
  editor = {Dale, Chris and Anderson, Tom},
  year = {2011},
  pages = {3--23},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-0-85729-133-2_1},
  abstract = {We introduce assured safety arguments, a new structure for arguing safety in which the safety argument is accompanied by a confidence argument that documents the confidence in the structure and bases of the safety argument. This structure separates the major components that have traditionally been confused within a single safety argument structure. Separation gives both arguments greater clarity of purpose, and helps avoid the introduction of superfluous arguments and evidence. In this paper we describe a systematic approach to establishing both arguments, illustrated with a running example.},
  isbn = {978-0-85729-132-5 978-0-85729-133-2},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/32C9VG2D/Hawkins et al. - 2011 - A New Approach to creating Clear Safety Arguments.pdf}
}

@article{hawkins2021,
  title = {Guidance on the {{Assurance}} of {{Machine Learning}} in {{Autonomous Systems}} ({{AMLAS}})},
  author = {Hawkins, Richard and Paterson, Colin and Picardi, Chiara and Jia, Yan and Calinescu, Radu and Habli, Ibrahim},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.01564 [cs]},
  eprint = {2102.01564},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Machine Learning (ML) is now used in a range of systems with results that are reported to exceed, under certain conditions, human performance. Many of these systems, in domains such as healthcare , automotive and manufacturing, exhibit high degrees of autonomy and are safety critical. Establishing justified confidence in ML forms a core part of the safety case for these systems. In this document we introduce a methodology for the Assurance of Machine Learning for use in Autonomous Systems (AMLAS). AMLAS comprises a set of safety case patterns and a process for (1) systematically integrating safety assurance into the development of ML components and (2) for generating the evidence base for explicitly justifying the acceptable safety of these components when integrated into autonomous system applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/cburr/Zotero/storage/VUYGUB4T/Hawkins et al_2021_Guidance on the Assurance of Machine Learning in Autonomous Systems (AMLAS).pdf}
}

@article{haynes2012,
  title = {Test, {{Learn}}, {{Adapt}}: {{Developing Public Policy}} with {{Randomised Controlled Trials}}},
  shorttitle = {Test, {{Learn}}, {{Adapt}}},
  author = {Haynes, Laura and Service, Owain and Goldacre, Ben and Torgerson, David},
  year = {2012},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2131581},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/SG2QRYNX/Haynes et al. - 2012 - Test, Learn, Adapt Developing Public Policy with .pdf}
}

@incollection{ho2015,
  title = {The {{Legal Concept}} of {{Evidence}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Ho, Hock Lai},
  editor = {Zalta, Edward N.},
  year = {2015},
  edition = {Winter 2015},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The legal concept of evidence is neither static nor universal.Medieval understandings of evidence in the age of trial by ordealwould be quite alien to modern sensibilities (Ho 2003\textendash 2004) andthere is no approach to evidence and proof that is shared by alllegal systems of the world today. Even within Western legaltraditions, there are significant differences between Anglo-Americanlaw and Continental European law (see Dama\v{s}ka 1973, 1975,1992, 1994, 1997). This entry focuses on the modern concept ofevidence that operates in the legal tradition to which Anglo-Americanlaw belongs.[1], It may seem obvious that there must be a legal concept ofevidence that is distinguishable from the ordinary concept ofevidence.  After all, there are in law many special rules on what canor cannot be introduced as evidence in court, on how evidence is to bepresented and the uses to which it may be put, on the strength orsufficiency of evidence needed to establish proof and so forth. Butthe law remains silent on some crucial matters. In resolving thefactual disputes before the court, the jury or, at a bench trial, thejudge has to rely on extra-legal principles. There have been academicattempts at systematic analysis of the operation of these principlesin legal fact-finding (Wigmore 1937; Anderson, Schum and Twining2009). These principles, so it is claimed, are of a general nature. Onthe basis that the logic in ``drawing inferences from evidence totest hypotheses and justify conclusions'' is governed by the sameprinciples across different disciplines (Twining and Hampsher-Monk2003: 4), ambitious projects have been undertaken to develop across-disciplinary framework for the analysis of evidence (Schum 1994)and to construct an interdisciplinary ``integrated science ofevidence'' (Dawid, Twining and Vasilaki 2011; cf. Tillers2008)., While evidential reasoning in law and in other contexts may sharecertain characteristics, there nevertheless remain aspects of theapproach to evidence and proof that are distinctive to law (Rescherand Joynt 1959). Section 1 (``conceptions of evidence'')identifies different meanings of evidence in legal discourse. Whenlawyers talk about evidence, what is it that they are referring to?What is it that they have in mind? Section 2 (``conditions forreceiving evidence'') approaches the concept of legal evidencefrom the angle of what counts as evidence in law. What are theconditions that the law imposes and must be met for something to bereceived by the court as evidence? Section 3 (``strength ofevidence'') shifts the attention to the stage where the evidencehas already been received by the court. Here the focus is on how thecourt weighs the evidence in reaching the verdict. In this connection,three properties of evidence will be discussed: probative value,sufficiency and degree of completeness.},
  keywords = {Bayes’ Theorem,epistemology,evidence,probability; interpretations of}
}

@book{holland2018,
  title = {The {{Dataset Nutrition Label}}: {{A Framework To Drive Higher Data Quality Standards}}},
  author = {Holland, Sarah and Hosny, Ahmed and Newman, Sarah and Joseph, Joshua and Chmielinski, Kasia},
  year = {2018},
  abstract = {Artificial intelligence (AI) systems built on incomplete or biased data will often exhibit problematic outcomes. Current methods of data analysis, particularly before model development, are costly and not standardized. The Dataset Nutrition Label1 (the Label) is a diagnostic framework that lowers the barrier to standardized data analysis by providing a distilled yet comprehensive overview of dataset ``ingredients'' before AI model development. Building a Label that can be applied across domains and data types requires that the framework itself be flexible and adaptable; as such, the Label is comprised of diverse qualitative and quantitative modules generated through multiple statistical and probabilistic modelling backends, but displayed in a standardized format. To demonstrate and advance this concept, we generated and published an open source prototype2 with seven sample modules on the ProPublica Dollars for Docs dataset. The benefits of the Label are manyfold. For data specialists, the Label will drive more robust data analysis practices, provide an efficient way to select the best dataset for their purposes, and increase the overall quality of AI models as a result of more robust training datasets and the ability to check for issues at the time of model development. For those building and publishing datasets, the Label creates an expectation of explanation, which will drive better data collection practices. We also explore the limitations of the Label, including the challenges of generalizing across diverse datasets, and the risk of using ``ground truth'' data as a comparison dataset. We discuss ways to move forward given the limitations identified. Lastly, we lay out future directions for the Dataset Nutrition Label project, including research and public policy agendas to further advance consideration of the concept.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/6KCLIPZ7/Hallinan et al. - 2020 - Data Protection and Privacy Data Protection and D.pdf}
}

@techreport{hopkins2020,
  title = {The {{Experimenter}}'s {{Inventory}}: {{A}} Catalogue of Experiments for Decision-Makers and Professionals},
  author = {Hopkins, Anna},
  year = {2020},
  month = jan,
  institution = {{Alliance for Useful Evidence}},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/S9LSQU59/Hopkins - The Experimenter's Inventory.pdf}
}

@misc{ibmresearch2018,
  title = {Introducing {{AI Fairness}} 360, {{A Step Towards Trusted AI}}},
  author = {IBM Research},
  year = {2018},
  month = sep,
  journal = {IBM Research Blog},
  abstract = {IBM Research announces AI Fairness 360, a comprehensive open-source toolkit of metrics and algorithms to check for and mitigate unwanted bias in AI.},
  copyright = {\textcopyright{} Copyright IBM Corp. 2020},
  howpublished = {https://www.ibm.com/blogs/research/2018/09/ai-fairness-360/},
  langid = {american}
}

@techreport{ico2020,
  title = {Guidance on the {{AI}} Auditing Framework},
  author = {ICO},
  year = {2020},
  institution = {{Information Commissioner's Office}},
  file = {/Users/cburr/Zotero/storage/BNPETCUZ/guidance-on-the-ai-auditing-framework-draft-for-consultation.pdf}
}

@book{joyce2019,
  title = {Measuring Impact by Design: A Guide to Methods for Impact Measurement.},
  shorttitle = {Measuring Impact by Design},
  author = {Joyce, Craig M and {Canada} and {Privy Council Office} and {Impact and Innovation Unit}},
  year = {2019},
  isbn = {978-0-660-29539-8},
  langid = {english},
  annotation = {OCLC: 1113389514},
  file = {/Users/cburr/Zotero/storage/N3QVBUDS/Joyce et al. - 2019 - Measuring impact by design a guide to methods for.pdf}
}

@article{kazim2020,
  title = {A {{Review}} of the {{UK-ICO}}'s {{Draft Guidance}} on the {{AI Auditing Framework}}},
  author = {Kazim, Emre and Koshiyama, Adriano},
  year = {2020},
  pages = {6},
  abstract = {The Information Commissioner's Office (ICO) timely call for consultation regarding their `Guidance on the AI auditing framework: Draft guidance for consultation' (February 2020) is part of a growing literature concerning the governance of Artificial Intelligence (AI) systems. The ICO's draft leads the UK's national conversation by producing guidance that encompasses both technical (ex. system impact assessments) and nontechnical (ex. human oversight) components to governance and represents a significant milestone in the movement towards standardising AI governance. Welcoming this crucial intervention, we summarise and critically evaluated each section of the draft guidance, offering feed-back in line with the call for consultation. We conclude with a note on what we anticipate will be future debates and by presenting our general recommendations.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/Q3BQ7WMV/Kazim and Koshiyama - 2020 - A Review of the UK-ICO’s Draft Guidance on the AI .pdf}
}

@article{kearns2008,
  title = {Reasons: {{Explanations}} or {{Evidence}}?},
  shorttitle = {Reasons},
  author = {Kearns, Stephen and Star, Daniel},
  year = {2008},
  month = oct,
  journal = {Ethics},
  volume = {119},
  number = {1},
  pages = {31--56},
  issn = {0014-1704, 1539-297X},
  doi = {10.1086/592587},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/L88RBA85/Kearns and Star - 2008 - Reasons Explanations or Evidence.pdf}
}

@phdthesis{kelly1998,
  title = {Arguing {{Safety}} \textendash{} {{A Systematic Approach}} to {{Managing Safety Cases}}},
  author = {Kelly, Timothy Patrick},
  year = {1998},
  address = {{Department of Computer Science}},
  langid = {english},
  school = {University of York},
  file = {/Users/cburr/Zotero/storage/L5ZQFHNN/Kelly - Arguing Safety – A Systematic Approach to Managing.pdf}
}

@inproceedings{kroll2021,
  title = {Outlining {{Traceability}}: {{A Principle}} for {{Operationalizing Accountability}} in {{Computing Systems}}},
  shorttitle = {Outlining {{Traceability}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Kroll, Joshua A.},
  year = {2021},
  month = mar,
  pages = {758--771},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3442188.3445937},
  abstract = {Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.},
  isbn = {978-1-4503-8309-7},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/NXG8SQWN/Kroll - 2021 - Outlining Traceability A Principle for Operationa.pdf}
}

@article{leslie2021,
  title = {The {{Arc}} of the {{Data Scientific Universe}}},
  author = {Leslie, David},
  year = {2021},
  month = jan,
  journal = {Harvard Data Science Review},
  doi = {10.1162/99608f92.938a18d7},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/VPVMPHHP/Leslie - 2021 - The Arc of the Data Scientific Universe.pdf}
}

@misc{lundberg2020,
  title = {Slundberg/Shap},
  author = {Lundberg, Scott},
  year = {2020},
  month = dec,
  abstract = {A game theoretic approach to explain the output of any machine learning model.},
  copyright = {MIT License         ,                 MIT License},
  keywords = {deep-learning,explainability,gradient-boosting,interpretability,machine-learning,shap,shapley}
}

@incollection{maksimov2018,
  title = {Two {{Decades}} of {{Assurance Case Tools}}: {{A Survey}}},
  shorttitle = {Two {{Decades}} of {{Assurance Case Tools}}},
  booktitle = {Developments in {{Language Theory}}},
  author = {Maksimov, Mike and Fung, Nick L. S. and Kokaly, Sahar and Chechik, Marsha},
  editor = {Hoshi, Mizuho and Seki, Shinnosuke},
  year = {2018},
  volume = {11088},
  pages = {49--59},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-99229-7_6},
  abstract = {In regulated safety-critical domains, such as the aerospace and nuclear domains, certification bodies often require systems to undergo a stringent safety assessment procedure to show their compliance to one or more safety standards. Assurance cases are an emerging way of communicating safety of a safety-critical system in a structured and comprehensive manner. Due to the significant complexity of the required materials, software tools are often used as a practical way of constructing assurance cases. This paper presents the first, to the best of our knowledge, systematic review of assurance case tools. Specifically, we provide a comprehensive list of assurance case tools developed over the past 20 years and an analysis of their functionalities.},
  isbn = {978-3-319-98653-1 978-3-319-98654-8},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/PQRL8JKW/Maksimov et al. - 2018 - Two Decades of Assurance Case Tools A Survey.pdf}
}

@article{miller2019,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  shorttitle = {Explanation in Artificial Intelligence},
  author = {Miller, Tim},
  year = {2019},
  journal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  doi = {10.1016/j.artint.2018.07.007},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/TSFKCD6X/1-s2.0-S0004370218305988-main.pdf;/Users/cburr/Zotero/storage/HMFXU9DQ/S0004370218305988.html}
}

@article{mitchell1997,
  title = {Toward a {{Theory}} of {{Stakeholder Identification}} and {{Salience}}: {{Defining}} the {{Principle}} of {{Who}} and {{What Really Counts}}},
  shorttitle = {Toward a {{Theory}} of {{Stakeholder Identification}} and {{Salience}}},
  author = {Mitchell, Ronald K. and Agle, Bradley R. and Wood, Donna J.},
  year = {1997},
  journal = {The Academy of Management Review},
  volume = {22},
  number = {4},
  pages = {853--886},
  publisher = {{Academy of Management}},
  issn = {0363-7425},
  doi = {10.2307/259247},
  abstract = {Stakeholder theory has been a popular heuristic for describing the management environment for years, but it has not attained full theoretical status. Our aim in this article is to contribute to a theory of stakeholder identification and salience based on stakeholders possessing one or more of three relationship attributes: power, legitimacy, and urgency. By combining these attributes, we generate a typology of stakeholders, propositions concerning their salience to managers of the firm, and research and management implications.},
  file = {/Users/cburr/Zotero/storage/P4TIAWSW/Mitchell et al. - 1997 - Toward a Theory of Stakeholder Identification and .pdf}
}

@article{mitchell2019,
  title = {Model {{Cards}} for {{Model Reporting}}},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  year = {2019},
  journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* '19},
  eprint = {1810.03993},
  eprinttype = {arxiv},
  pages = {220--229},
  doi = {10.1145/3287560.3287596},
  abstract = {Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/cburr/Zotero/storage/8PXIGIR8/Mitchell et al. - 2019 - Model Cards for Model Reporting.pdf}
}

@article{mokander2021,
  title = {Ethics-{{Based Auditing}} to {{Develop Trustworthy AI}}},
  author = {M{\"o}kander, Jakob and Floridi, Luciano},
  year = {2021},
  month = feb,
  journal = {Minds and Machines},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-021-09557-8},
  abstract = {A series of recent developments points towards auditing as a promising mechanism to bridge the gap between principles and practice in AI ethics. Building on ongoing discussions concerning ethics-based auditing, we offer three contributions. First, we argue that ethics-based auditing can improve the quality of decision making, increase user satisfaction, unlock growth potential, enable law-making, and relieve human suffering. Second, we highlight current best practices to support the design and implementation of ethics-based auditing: To be feasible and effective, ethics-based auditing should take the form of a continuous and constructive process, approach ethical alignment from a system perspective, and be aligned with public policies and incentives for ethically desirable behaviour. Third, we identify and discuss the constraints associated with ethics-based auditing. Only by understanding and accounting for these constraints can ethics-based auditing facilitate ethical alignment of AI, while enabling society to reap the full economic and social benefits of automation.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/XJH9ZMYR/Mökander and Floridi - 2021 - Ethics-Based Auditing to Develop Trustworthy AI.pdf}
}

@article{moss2020,
  title = {Governing with {{Algorithmic Impact Assessments}}: {{Six Observations}}},
  shorttitle = {Governing with {{Algorithmic Impact Assessments}}},
  author = {Moss, Emanuel and Watkins, Elizabeth and Metcalf, Jacob and Elish, Madeleine Clare},
  year = {2020},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3584818},
  abstract = {Algorithmic impact assessments (AIA) are increasingly being proposed as a mechanism for algorithmic accountability. These assessments are seen as potentially useful for anticipating, avoiding, and mitigating the negative consequences of algorithmic decision-making systems (ADS). At the same time, what an AIA would entail remains under-specified. While promising, AIAs raise as many questions as they answer. Choices about the methods, scope, and purpose of impact assessments structure the possible governance outcomes. Decisions about what type of effects count as an impact, when impacts are assessed, whose interests are considered, who is invited to participate, who conducts the assessment, the public availability of the assessment, and what the outputs of the assessment might be all shape the forms of accountability that AIA proponents seek to encourage. These considerations remain open, and will determine whether and how AIAs can function as a viable governance mechanism in the broader algorithmic accountability toolkit, especially with regard to furthering the public interest. Because AlAs are still an incipient governance strategy, approaching them as social constructions that do not require a single or universal approach offers a chance to produce interventions that emerge from careful deliberation.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/NF49A3KG/Moss et al. - 2020 - Governing with Algorithmic Impact Assessments Six.pdf}
}

@book{owen2013,
  title = {Responsible Innovation},
  editor = {Owen, Richard and Bessant, J. R. and Heintz, Maggy},
  year = {2013},
  publisher = {{Wiley}},
  address = {{Chichester, West Sussex, United Kingdom}},
  isbn = {978-1-118-55140-0 978-1-118-55139-4 978-1-118-55141-7},
  langid = {english},
  lccn = {HD45},
  keywords = {Environmental aspects,Moral and ethical aspects,New products,Research; Industrial,Technological innovations},
  file = {/Users/cburr/Zotero/storage/TSZMVCYZ/Owen et al. - 2013 - Responsible innovation.pdf}
}

@misc{pair2020,
  title = {What-{{If Tool}} - {{People}} + {{AI Research}} ({{PAIR}})},
  author = {PAIR},
  year = {2020},
  howpublished = {https://pair-code.github.io/what-if-tool/}
}

@article{phillips2018,
  title = {Face Recognition Accuracy of Forensic Examiners, Superrecognizers, and Face Recognition Algorithms},
  author = {Phillips, P. Jonathon and Yates, Amy N. and Hu, Ying and Hahn, Carina A. and Noyes, Eilidh and Jackson, Kelsey and Cavazos, Jacqueline G. and Jeckeln, G{\'e}raldine and Ranjan, Rajeev and Sankaranarayanan, Swami and Chen, Jun-Cheng and Castillo, Carlos D. and Chellappa, Rama and White, David and O'Toole, Alice J.},
  year = {2018},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {24},
  pages = {6171--6176},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1721355115},
  abstract = {Achieving the upper limits of face identification accuracy in forensic applications can minimize errors that have profound social and personal consequences. Although forensic examiners identify faces in these applications, systematic tests of their accuracy are rare. How can we achieve the most accurate face identification: using people and/or machines working alone or in collaboration? In a comprehensive comparison of face identification by humans and computers, we found that forensic facial examiners, facial reviewers, and superrecognizers were more accurate than fingerprint examiners and students on a challenging face identification test. Individual performance on the test varied widely. On the same test, four deep convolutional neural networks (DCNNs), developed between 2015 and 2017, identified faces within the range of human accuracy. Accuracy of the algorithms increased steadily over time, with the most recent DCNN scoring above the median of the forensic facial examiners. Using crowd-sourcing methods, we fused the judgments of multiple forensic facial examiners by averaging their rating-based identity judgments. Accuracy was substantially better for fused judgments than for individuals working alone. Fusion also served to stabilize performance, boosting the scores of lower-performing individuals and decreasing variability. Single forensic facial examiners fused with the best algorithm were more accurate than the combination of two examiners. Therefore, collaboration among humans and between humans and machines offers tangible benefits to face identification accuracy in important applications. These results offer an evidence-based roadmap for achieving the most accurate face identification possible.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/PID8IB34/Phillips et al. - 2018 - Face recognition accuracy of forensic examiners, s.pdf}
}

@incollection{picardi2019,
  title = {A {{Pattern}} for {{Arguing}} the {{Assurance}} of {{Machine Learning}} in {{Medical Diagnosis Systems}}},
  booktitle = {Computer {{Safety}}, {{Reliability}}, and {{Security}}},
  author = {Picardi, Chiara and Hawkins, Richard and Paterson, Colin and Habli, Ibrahim},
  editor = {Romanovsky, Alexander and Troubitsyna, Elena and Bitsch, Friedemann},
  year = {2019},
  volume = {11698},
  pages = {165--179},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-26601-1_12},
  abstract = {Machine Learning offers the potential to revolutionise healthcare with recent work showing that machine-learned algorithms can achieve or exceed expert human performance. The adoption of such systems in the medical domain should not happen, however, unless sufficient assurance can be demonstrated. In this paper we consider the implicit assurance argument for state-of-the-art systems that uses machine-learnt models for clinical diagnosis, e.g. retinal disease diagnosis. Based upon an assessment of this implicit argument we identify a number of additional assurance considerations that would need to be addressed in order to create a compelling assurance case. We present an assurance case pattern that we have developed to explicitly address these assurance considerations. This pattern may also have the potential to be applied to a wide class of critical domains where ML is used in the decision making process.},
  isbn = {978-3-030-26600-4 978-3-030-26601-1},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/6I5IYAKA/Picardi et al. - 2019 - A Pattern for Arguing the Assurance of Machine Lea.pdf}
}

@inproceedings{picardi2020,
  title = {Assurance {{Argument Patterns}} and {{Processes}} for {{Machine Learning}} in {{Safety-Related Systems}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Artificial Intelligence Safety}} ({{SafeAI}} 2020)},
  author = {Picardi, Chiara and Paterson, Colin and Hawkins, Richard and Calinescu, Radu and Habli, Ibrahim},
  year = {2020},
  month = feb,
  series = {{{CEUR Workshop Proceedings}}},
  pages = {23--30},
  publisher = {{CEUR Workshop Proceedings}},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/3JBGY455/Picardi et al. - Assurance Argument Patterns and Processes for Mach.pdf}
}

@techreport{pyper2020,
  type = {Briefing {{Paper}}},
  title = {The {{Public Sector Equality Duty}} and {{Equality Impact Assessments}}},
  author = {Pyper, Doug},
  year = {2020},
  month = jul,
  number = {06591},
  institution = {{House of Commons Library}},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/NKRPEGD2/Pyper - The Public Sector Equality Duty and Equality Impac.pdf}
}

@article{raji2020,
  title = {Closing the {{AI Accountability Gap}}: {{Defining}} an {{End-to-End Framework}} for {{Internal Algorithmic Auditing}}},
  author = {Raji, Inioluwa Deborah and Smart, Andrew and White, Rebecca N and Mitchell, Margaret and Gebru, Timnit and Hutchinson, Ben and {Smith-Loud}, Jamila and Theron, Daniel and Barnes, Parker},
  year = {2020},
  pages = {12},
  abstract = {Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/UARF4HH9/Raji et al. - 2020 - Closing the AI Accountability Gap Defining an End.pdf}
}

@article{reijers2020,
  title = {Responsible Innovation between Virtue and Governance: Revisiting {{Arendt}}'s Notion of Work as Action},
  shorttitle = {Responsible Innovation between Virtue and Governance},
  author = {Reijers, Wessel},
  year = {2020},
  month = sep,
  journal = {Journal of Responsible Innovation},
  volume = {7},
  number = {3},
  pages = {471--489},
  issn = {2329-9460},
  doi = {10.1080/23299460.2020.1806524},
  abstract = {Recent developments in responsible innovation have focused on the governance of innovation processes. The dimension of virtue in innovation processes has thereby been largely overlooked, and more significantly the constitutive relation between virtue and governance that enables responsible innovation. To understand responsible innovation in terms of this relation, this paper turns to Hannah Arendt's ontology of the Vita Activa. First, it problematises responsible innovation in Arendt's work, but then points at a hitherto undiscussed possibility of responsible innovation as `work in the mode of action'. Second, it explores this possibility as it arises out of nine modes for human activity in Arendt's work, arguing that it constitutes a hybrid activity between world and plurality, durability and fragility, and the firm and the public sphere. Third, it explains how the `web of stories' links virtuous action and governance, which points at a novel understanding of the role of narrative for responsible innovation.},
  keywords = {Arendt,governance,narrative,responsible innovation,virtue},
  file = {/Users/cburr/Zotero/storage/A7R6IYHI/Reijers - 2020 - Responsible innovation between virtue and governan.pdf}
}

@article{saltelli2020,
  title = {Five Ways to Ensure That Models Serve Society: A Manifesto},
  shorttitle = {Five Ways to Ensure That Models Serve Society},
  author = {Saltelli, Andrea and Bammer, Gabriele and Bruno, Isabelle and Charters, Erica and Di Fiore, Monica and Didier, Emmanuel and Nelson Espeland, Wendy and Kay, John and Lo Piano, Samuele and Mayo, Deborah and Pielke Jr, Roger and Portaluri, Tommaso and Porter, Theodore M. and Puy, Arnald and Rafols, Ismael and Ravetz, Jerome R. and Reinert, Erik and Sarewitz, Daniel and Stark, Philip B. and Stirling, Andrew and {van der Sluijs}, Jeroen and Vineis, Paolo},
  year = {2020},
  month = jun,
  journal = {Nature},
  volume = {582},
  number = {7813},
  pages = {482--484},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/d41586-020-01812-9},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/9JYBDJIQ/Saltelli et al. - 2020 - Five ways to ensure that models serve society a m.pdf}
}

@article{schuijff2020,
  title = {Practices of {{Responsible Research}} and {{Innovation}}: {{A Review}}},
  shorttitle = {Practices of {{Responsible Research}} and {{Innovation}}},
  author = {Schuijff, Mirjam and Dijkstra, Anne M.},
  year = {2020},
  month = apr,
  journal = {Science and Engineering Ethics},
  volume = {26},
  number = {2},
  pages = {533--574},
  issn = {1353-3452, 1471-5546},
  doi = {10.1007/s11948-019-00167-3},
  abstract = {This paper presents results of a systematic literature review of RRI practices which aimed to gather insights to further both the theoretical and practical development of RRI. Analysing practices of RRI and mapping out main approaches as well as the values, dimensions or characteristics pursued with those practices, can add to understanding of the more conceptual discussions of RRI and enhance the academic debate. The results, based on a corpus of 52 articles, show that practices already reflect the rich variety of values, dimensions and characteristics provided in the main definitions in use, although not all are addressed yet. In fact, articles dealing with uptake of RRI practices may be improved by including more methodological information. RRI practices may further the conceptual debate by including more reflection, and these may foster mutual responsiveness between theory and practice by early anticipating impacts.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/LPDKSK7V/Schuijff and Dijkstra - 2020 - Practices of Responsible Research and Innovation .pdf}
}

@article{sloane2020,
  title = {Participation Is Not a {{Design Fix}} for {{Machine Learning}}},
  author = {Sloane, Mona and Moss, Emanuel and Awomolo, Olaitan and Forlano, Laura},
  year = {2020},
  month = aug,
  journal = {arXiv:2007.02423 [cs]},
  eprint = {2007.02423},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper critically examines existing modes of participation in design practice and machine learning. Cautioning against 'participation-washing', it suggests that the ML community must become attuned to possibly exploitative and extractive forms of community involvement and shift away from the prerogatives of context-independent scalability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/cburr/Zotero/storage/5BERKGRX/Sloane et al. - 2020 - Participation is not a Design Fix for Machine Lear.pdf;/Users/cburr/Zotero/storage/LHZC3UVE/2007.html}
}

@article{sujan2019,
  title = {Human Factors Challenges for the Safe Use of Artificial Intelligence in Patient Care},
  author = {Sujan, Mark and Furniss, Dominic and Grundy, Kath and Grundy, Howard and Nelson, David and Elliott, Matthew and White, Sean and Habli, Ibrahim and Reynolds, Nick},
  year = {2019},
  month = nov,
  journal = {BMJ Health \& Care Informatics},
  volume = {26},
  number = {1},
  pages = {e100081},
  issn = {2632-1009},
  doi = {10.1136/bmjhci-2019-100081},
  abstract = {The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/G2455GBK/Sujan et al. - 2019 - Human factors challenges for the safe use of artif.pdf}
}

@book{sweenor2020,
  title = {{{ML Ops}}: {{Operationalizing Data Science}}},
  shorttitle = {{{ML Ops}}},
  author = {Sweenor, David and Hillion, Steven and Rope, Dan and Kannabiran, Dev and Hill, Thomas and O'Connell, Michael and Safari, an O'Reilly Media Company},
  year = {2020},
  abstract = {More than half of the analytics and machine learning (ML) models created by organizations today never make it into production. Instead, many of these ML models do nothing more than provide static insights in a slideshow. If they aren't truly operational, these models can't possibly do what you've trained them to do. This report introduces practical concepts to help data scientists and application engineers operationalize ML models to drive real business change. Through lessons based on numerous projects around the world, six experts in data analytics provide an applied four-step approach-Build, Manage, Deploy and Integrate, and Monitor-for creating ML-infused applications within your organization. You'll learn how to: Fulfill data science value by reducing friction throughout ML pipelines and workflows Constantly refine ML models through retraining, periodic tuning, and even complete remodeling to ensure long-term accuracy Design the ML Ops lifecycle to ensure that people-facing models are unbiased, fair, and explainable Operationalize ML models not only for pipeline deployment but also for external business systems that are more complex and less standardized Put the four-step Build, Manage, Deploy and Integrate, and Monitor approach into action.},
  langid = {english},
  annotation = {OCLC: 1152552126}
}

@techreport{theassurancecaseworkinggroup2018,
  title = {{{GSN Community Standard}} ({{Version}} 2)},
  author = {The Assurance Case Working Group},
  year = {2018},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/7G679MTA/2018 - GSN Community Standard.PDF}
}

@book{toulmin2003,
  title = {The {{Uses}} of {{Argument}}, {{Updated Edition}}},
  author = {Toulmin, Stephen},
  year = {2003},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-0-511-06271-1},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/VIN8XWPN/Toulmin - The Uses of Argument, Updated Edition.pdf}
}

@article{visser2020,
  title = {Annotating {{Argument Schemes}}},
  author = {Visser, Jacky and Lawrence, John and Reed, Chris and Wagemans, Jean and Walton, Douglas},
  year = {2020},
  month = may,
  journal = {Argumentation},
  issn = {0920-427X, 1572-8374},
  doi = {10.1007/s10503-020-09519-x},
  abstract = {Argument schemes are abstractions substantiating the inferential connection between premise(s) and conclusion in argumentative communication. Identifying such conventional patterns of reasoning is essential to the interpretation and evaluation of argumentation. Whether studying argumentation from a theory-driven or data-driven perspective, insight into the actual use of argumentation in communicative practice is essential. Large and reliably annotated corpora of argumentative discourse to quantitatively provide such insight are few and far between. This is all the more true for argument scheme corpora, which tend to suffer from a combination of limited size, poor validation, and the use of ad hoc restricted typologies. In the current paper, we describe the annotation of schemes on the basis of two distinct classifications: Walton's taxonomy of argument schemes, and Wagemans' Periodic Table of Arguments. We describe the annotation procedure for each, and the quantitative characteristics of the resulting annotated text corpora. In doing so, we extend the annotation of the preexisting US2016 corpus of televised election debates, resulting in, to the best of our knowledge, the two largest consistently annotated corpora of schemes in argumentative dialogue publicly available. Based on evaluation in terms of inter-annotator agreement, we propose further improvements to the guidelines for annotating schemes: the argument scheme key, and the Argument Type Identification Procedure.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/FNJ2ARA9/Visser et al. - 2020 - Annotating Argument Schemes.pdf}
}

@book{vonschomberg2011,
  title = {Towards {{Responsible Research}} and {{Innovation}} in the {{Information}} and {{Communication Technologies}} and {{Security Technologies Fields}}},
  editor = {Von Schomberg, Ren{\'e}},
  year = {2011},
  publisher = {{Publications Office of the European Union}},
  isbn = {978-92-79-20404-3}
}

@book{walton2008,
  title = {Argumentation {{Schemes}}},
  author = {Walton, Douglas and Reed, Chris and Macagno, Fabrizio},
  year = {2008},
  publisher = {{Cambridge University Press}},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/ZS4NH8PT/Walton et al. - Argumentation Schemes.pdf}
}

@incollection{walton2009,
  title = {Argumentation {{Theory}}: {{A Very Short Introduction}}},
  shorttitle = {Argumentation {{Theory}}},
  booktitle = {Argumentation in {{Artificial Intelligence}}},
  author = {Walton, Douglas},
  editor = {Simari, Guillermo and Rahwan, Iyad},
  year = {2009},
  pages = {1--22},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-98197-0_1},
  isbn = {978-0-387-98196-3 978-0-387-98197-0},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/CPJK5FP5/Walton - 2009 - Argumentation Theory A Very Short Introduction.pdf}
}

@incollection{ward2020,
  title = {An {{Assurance Case Pattern}} for the {{Interpretability}} of {{Machine Learning}} in {{Safety-Critical Systems}}},
  booktitle = {Computer {{Safety}}, {{Reliability}}, and {{Security}}. {{SAFECOMP}} 2020 {{Workshops}}},
  author = {Ward, Francis Rhys and Habli, Ibrahim},
  editor = {Casimiro, Ant{\'o}nio and Ortmeier, Frank and Schoitsch, Erwin and Bitsch, Friedemann and Ferreira, Pedro},
  year = {2020},
  volume = {12235},
  pages = {395--407},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-55583-2_30},
  abstract = {Machine Learning (ML) has the potential to become widespread in safety-critical applications. It is therefore important that we have sufficient confidence in the safe behaviour of the ML-based functionality. One key consideration is whether the ML being used is interpretable. In this paper, we present an argument pattern, i.e. reusable structure, that can be used for justifying the sufficient interpretability of ML within a wider assurance case. The pattern can be used to assess whether the right interpretability method and format are used in the right context (time, setting and audience). This argument structure provides a basis for developing and assessing focused requirements for the interpretability of ML in safety-critical domains.},
  isbn = {978-3-030-55582-5 978-3-030-55583-2},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/H8SF5W96/Ward and Habli - 2020 - An Assurance Case Pattern for the Interpretability.pdf}
}

@article{young2019,
  title = {Toward Inclusive Tech Policy Design: A Method for Underrepresented Voices to Strengthen Tech Policy Documents},
  shorttitle = {Toward Inclusive Tech Policy Design},
  author = {Young, Meg and Magassa, Lassana and Friedman, Batya},
  year = {2019},
  month = jun,
  journal = {Ethics and Information Technology},
  volume = {21},
  number = {2},
  pages = {89--103},
  issn = {1388-1957, 1572-8439},
  doi = {10.1007/s10676-019-09497-z},
  abstract = {To be successful, policy must anticipate a broad range of constituents. Yet, all too often, technology policy is written with primarily mainstream populations in mind. In this article, drawing on Value Sensitive Design and discount evaluation methods, we introduce a new method\textemdash Diverse Voices\textemdash for strengthening pre-publication technology policy documents from the perspective of underrepresented groups. Cost effective and high impact, the Diverse Voices method intervenes by soliciting input from ``experiential'' expert panels (i.e., members of a particular stakeholder group and/or those serving that group). We first describe the method. Then we report on two case studies demonstrating its use: one with a white paper on augmented reality technology with expert panels on people with disabilities, people who were formerly or currently incarcerated, and women; and the other with a strategy document on automated driving vehicle technologies with expert panels on youth, non-car drivers, and extremely low-income people. In both case studies, panels identified significant shortcomings in the pre-publication documents which, if addressed, would mitigate some of the disparate impact of the proposed policy recommendations on these particular stakeholder groups. Our discussion includes reflection on the method, evidence for its success, its limitations, and future directions.},
  langid = {english},
  file = {/Users/cburr/Zotero/storage/VYFNVNLL/Young et al. - 2019 - Toward inclusive tech policy design a method for .pdf}
}

@misc{zotero-9416,
  title = {Why Is Ethical Assurance Important?},
  howpublished = {http://127.0.0.1:4000/guides/why-is-ethical-assurance-important.html},
  file = {/Users/cburr/Zotero/storage/NCU4IE89/why-is-ethical-assurance-important.html}
}


